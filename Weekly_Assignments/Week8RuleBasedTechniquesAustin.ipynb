{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "027b611b-e22f-468f-ad40-b5b2088d8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from spacy.lang.en import English\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def matching_curly_braces(text):\n",
    "    \"\"\"return True if text has matching curly braces\n",
    "    in the right order. Otherwise, returns False.\"\"\"\n",
    "    stack = []\n",
    "    for c in text:\n",
    "        if c == '{':\n",
    "            stack.append(c)\n",
    "        elif c == '}':\n",
    "            if not stack or (c == '}' and stack[-1] != '{'):\n",
    "                break\n",
    "            stack.pop()\n",
    "    return not stack\n",
    "\n",
    "\n",
    "def remove_curly_braces(text):\n",
    "    \"\"\"remove all characters enclosed in curly braces from text\"\"\"\n",
    "    left_bracket = []\n",
    "    right_bracket = []\n",
    "    stack = []\n",
    "    for i, c in enumerate(text):\n",
    "        if c == '{':  # c is curly brace\n",
    "            stack.append(c)\n",
    "            left_bracket.append(i)\n",
    "        elif c == '}':           # c is closed curly brace\n",
    "            # string is not valid\n",
    "            if not stack or \\\n",
    "                (c == '}' and stack[-1] != '{'):\n",
    "                break\n",
    "            stack.pop() # pop open curly brace\n",
    "            right_bracket.append(i+1)\n",
    "    for _ in range(len(left_bracket)):\n",
    "        shift = 0\n",
    "        left_index = 0\n",
    "        right_index = 0\n",
    "        for left_index in range(len(left_bracket)):\n",
    "            if left_bracket[left_index] > right_bracket[right_index]:\n",
    "                left_index -= 1\n",
    "                break\n",
    "        text = text[0:left_bracket[left_index]] + text[right_bracket[right_index]:]\n",
    "        shift = right_bracket[right_index] - left_bracket[left_index]\n",
    "        for n in range(len(left_bracket)):\n",
    "            if right_bracket[n] > right_bracket[right_index]:\n",
    "                right_bracket[n] -= shift \n",
    "            if left_bracket[n] > left_bracket[left_index]:\n",
    "                left_bracket[n] -= shift \n",
    "        left_bracket.pop(left_index)\n",
    "        right_bracket.pop(right_index)\n",
    "    return text\n",
    "\n",
    "def remove_smart_quotes(text):\n",
    "    \"\"\"remove quotes in text\"\"\"\n",
    "    return text.replace(\"“\", \"\\\"\").replace(\"”\",\"\\\"\")\n",
    "\n",
    "def clean_data(input_name, output_name):\n",
    "    # read input file\n",
    "    if input_name != \"data.txt\":\n",
    "        with open(input_name, \"r\", encoding=\"utf8\") as input_file:\n",
    "            text = input_file.readlines()\n",
    "            alt_text = \"\"\n",
    "            for line in text:\n",
    "                alt_text += line + \"\\n\"\n",
    "            if matching_curly_braces(alt_text):\n",
    "                # remove text enclosed in curly_braces to remove random html code\n",
    "                alt_text = remove_curly_braces(alt_text)\n",
    "                text = alt_text\n",
    "    else:\n",
    "        with open(input_name, \"r\") as input_file:\n",
    "            text = input_file.readlines()\n",
    "            alt_text = \"\"\n",
    "            for line in text:\n",
    "                alt_text += line + \"\\n\"\n",
    "            if matching_curly_braces(alt_text):\n",
    "                # remove text enclosed in curly_braces to remove random html code\n",
    "                alt_text = remove_curly_braces(alt_text)\n",
    "                text = alt_text\n",
    "\n",
    "    text = re.sub(\"U.S.\", \"United States \", text)\n",
    "    text = re.sub(\"p.m.\", \"pm \", text)\n",
    "    text = re.sub(\"a.m.\", \"am \", text)\n",
    "    text = re.sub(\"E.A.T.\", \"eat \", text)\n",
    "    text = re.sub(\"-\", \" \", text)\n",
    "    text = re.sub(r'([A-Za-z])(\\d)', r'\\1 \\2', text)\n",
    "    text = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # split text into lines based on if it has a period, question, exclamation, or newline character\n",
    "    text = re.split(r'[.?!]|\\\\n', text)\n",
    "\n",
    "    # set list of stopwords\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    html_stopwords = [\"var\",\"https\", \"csrftoken\", \"userdata\", \"csmllty54qx20erutnfcgs839jd2y\", \"const\", \"saml\",\"getitem\", \"firebaseat\", \"firebaseapp\", \"json\"]\n",
    "    stopwords.update(html_stopwords)\n",
    "    # write to output file\n",
    "    with open(output_name, \"w\") as output_file:\n",
    "        for line in text:\n",
    "            # remove punctuation and stopwords from line\n",
    "            tokenized_line = [word.lower() for word in word_tokenize(line) if word.isalnum() and word.lower() not in stopwords]\n",
    "            newline = \"\"\n",
    "            for word in tokenized_line:\n",
    "                newline += word + \" \"\n",
    "            if newline.strip():\n",
    "                output_file.write(newline.strip() + \"\\n\")\n",
    "\n",
    "# clean_data(\"all_data.txt\", \"all_cleaned_data.txt\")\n",
    "clean_data(\"data.txt\", \"cleaned_data.txt\")\n",
    "# clean_data(\"test.txt\", \"cleaned_test.txt\")\n",
    "# clean_data(\"data2.txt\", \"cleaned_data2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a3bd9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the WordNet list of chosen_word\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "chosen_words = [\"students\", \"program\", \"pacific\", \"learn\", \"university\"]\n",
    "\n",
    "for chosen_word in chosen_words:\n",
    "    definitions = wn.synsets(chosen_word)\n",
    "    if len(definitions) > 0:\n",
    "        word_list = definitions[0].hyponyms()\n",
    "        simple_names = []\n",
    "        for word in range (len(word_list)):\n",
    "            simple_name = word_list[word].lemma_names()[0]\n",
    "            simple_names.append(simple_name)\n",
    "\n",
    "        # generate some sample data\n",
    "\n",
    "        with open(\"cleaned_data.txt\", \"r\") as input_file:\n",
    "            lines = input_file.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                index = -1\n",
    "                index = line.find(chosen_word, index + 1)\n",
    "                if index != -1:\n",
    "                    with open(\"generated_data.txt\", \"a\") as output_file:\n",
    "                        for val in range(len(simple_names)):\n",
    "                            output_file.write(line[0:index] + simple_names[val] + line[index+len(chosen_word):] + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9143b7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det these) (N links))\n",
      "  (VP\n",
      "    (VP (V are) (V provided))\n",
      "    (PP (P for) (NP (Det your) (N information)))))\n"
     ]
    }
   ],
   "source": [
    "# an NLTK CFG grammar \n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N N |Det N PP | Pro | Adj N\n",
    "Pro -> 'I' |'you'|'we'\n",
    "VP -> V NP | VP PP | VP Adv | V V\n",
    "Det -> 'an' | 'my' | 'the' | 'these'|'your'\n",
    "N -> 'permit' | 'areas' | 'parking' |'links' | 'information' |'children'\n",
    "V -> 'saw'|'watched'|'have'|'can'|'park'|'are'|'provided'\n",
    "P -> 'for'|'to'|'where'|'here'\n",
    "\"\"\")\n",
    "# parse and visualize a sentence\n",
    "# we will need this to tokenize the input\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "# a package for visualizing parse trees\n",
    "import svgling\n",
    "# to use svgling we need to disable NLTK's normal visualization functions\n",
    "svgling.disable_nltk_png()\n",
    "# example sentence that can be parsed with the grammar we've defined\n",
    "example = \"these links are provided for your information\"\n",
    "# with open(\"cleaned_data.txt\", \"r\") as input_file:\n",
    "#     example = input_file.readlines()[10]\n",
    "sent = word_tokenize(example)\n",
    "# create a chart parser based on the grammar above\n",
    "parser = nltk.ChartParser(grammar)\n",
    "# parse the sentence\n",
    "trees = list(parser.parse(sent))\n",
    "# print a text-formatted parse tree\n",
    "print(trees[0])\n",
    "# print an SVG formatted parse tree\n",
    "trees[0]\n",
    "with open(\"CFG_Parsed_Sentence.txt\", \"w\") as output_file:\n",
    "    output_file.write(str(trees[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39ff1224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('parking', 'EVENT'), ('close by', 'LOCATION'), ('new student', 'STUDENT'), ('orientation', 'EVENT'), ('stockton', 'CAMPUS')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "student_patterns = [\n",
    "    {\"label\": \"STUDENT\", \"pattern\": \"undergraduate\"},\n",
    "    {\"label\": \"STUDENT\", \"pattern\": \"graduate\"},\n",
    "    {\"label\": \"STUDENT\", \"pattern\": \"international\"},\n",
    "    {\"label\": \"STUDENT\", \"pattern\": \"new student\"}]\n",
    "event_patterns = [\n",
    "    {\"label\": \"EVENT\", \"pattern\": \"orientation\"},\n",
    "    {\"label\": \"EVENT\", \"pattern\": \"parking\"}]\n",
    "vehicle_patterns = [\n",
    "    {\"label\": \"VEHICLE\", \"pattern\": \"car\"},\n",
    "    {\"label\": \"VEHICLE\", \"pattern\": \"bus\"},\n",
    "    {\"label\": \"VEHICLE\", \"pattern\": \"shuttle\"},\n",
    "    {\"label\": \"VEHICLE\", \"pattern\": \"bike\"},\n",
    "    {\"label\": \"VEHICLE\", \"pattern\": \"bicycle\"}]\n",
    "campus_patterns = [\n",
    "    {\"label\": \"CAMPUS\", \"pattern\": \"stockton\"},\n",
    "    {\"label\": \"CAMPUS\", \"pattern\": \"san francisco\"},\n",
    "    {\"label\": \"CAMPUS\", \"pattern\": \"sacramento\"}]\n",
    "building_patterns = [\n",
    "    {\"label\": \"BUILDING\", \"pattern\": \"biology\"},\n",
    "    {\"label\": \"BUILDING\", \"pattern\": \"chemistry\"},\n",
    "    {\"label\": \"BUILDING\", \"pattern\": \"pharmacy\"},\n",
    "    {\"label\": \"BUILDING\", \"pattern\": \"computer science\"},\n",
    "    {\"label\": \"BUILDING\", \"pattern\": \"library\"},\n",
    "    {\"label\": \"BUILDING\", \"pattern\": \"cafeteria\"}]\n",
    "location_patterns = [\n",
    "    {\"label\": \"LOCATION\", \"pattern\": \"near here\",\"id\":\"nearby\"},\n",
    "    {\"label\": \"LOCATION\", \"pattern\": \"close by\",\"id\":\"nearby\"},\n",
    "    {\"label\": \"LOCATION\", \"pattern\": \"near me\",\"id\":\"nearby\"},\n",
    "    {\"label\": \"LOCATION\", \"pattern\": \"walking distance\", \"id\":\"short_walk\"},\n",
    "    {\"label\": \"LOCATION\", \"pattern\": \"short walk\", \"id\":\"short_walk\"},\n",
    "    {\"label\": \"LOCATION\", \"pattern\": \"a short drive\"}]\n",
    "           \n",
    "ruler.add_patterns(student_patterns)\n",
    "ruler.add_patterns(event_patterns)\n",
    "ruler.add_patterns(campus_patterns)\n",
    "ruler.add_patterns(vehicle_patterns)\n",
    "ruler.add_patterns(building_patterns)\n",
    "ruler.add_patterns(location_patterns)\n",
    "\n",
    "doc = nlp(\"where can i find a parking lot close by the new student orientation on the stockton campus?\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7734d223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">where can i find a \n",
       "<mark class=\"entity\" style=\"background: #baffc9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    parking\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " lot \n",
       "<mark class=\"entity\" style=\"background: #fedcba; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    close by\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOCATION</span>\n",
       "</mark>\n",
       " the \n",
       "<mark class=\"entity\" style=\"background: #ea7e7e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    new student\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #baffc9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    orientation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " on the \n",
       "<mark class=\"entity\" style=\"background: #ffffaa; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    stockton\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CAMPUS</span>\n",
       "</mark>\n",
       " campus?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "colors = {\"STUDENT\": \"#ea7e7e\",\n",
    "          \"EVENT\": \"#baffc9\",\n",
    "          \"VEHICLE\": \"#abcdef\",\n",
    "          \"BUILDING\": \"#aaffbb\",\n",
    "          \"CAMPUS\": \"#ffffaa\",\n",
    "          \"LOCATION\": \"#fedcba\"}\n",
    "options = {\"ents\": [\"STUDENT\", \"EVENT\", \"VEHICLE\", \"BUILDING\", \"CAMPUS\", \"LOCATION\"], \"colors\": colors}\n",
    "svg = displacy.render(doc, style=\"ent\", options=options,jupyter = False)\n",
    "with open(\"slot_filling_and_visualization.svg\", \"w\") as output_file:\n",
    "    output_file.write(svg)\n",
    "displacy.render(doc, style=\"ent\", options=options,jupyter = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
