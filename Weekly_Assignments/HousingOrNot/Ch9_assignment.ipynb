{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf987640-488e-4479-9d1c-d88f277ff4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79516c85-077b-4c2e-8463-4b42b197ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data file by removing non-text (e.g. emojis, smart quotes) and regularizing text \n",
    "#(e.g. tokenization, lower casing, stemming, lemmatizing, POS tagging, stop word removal, removing punctuation, spelling correction)\n",
    "import demoji\n",
    "import json\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "path= \"faqsFromPdf.json\"\n",
    "with open(path, 'r') as json_file:\n",
    "    text = json.load(json_file)\n",
    "\n",
    "categories = []\n",
    "all_text = \"\"\n",
    "for key, faq_list in text.items():\n",
    "    group_text = \"\"\n",
    "\n",
    "    for faq_item in faq_list:\n",
    "        all_text += faq_item[\"question\"] + \" \" + faq_item[\"answer\"] + \" \"\n",
    "        group_text += faq_item[\"question\"] + faq_item[\"answer\"] \n",
    "    categories.append(group_text)\n",
    "\n",
    "# Removing emojis\n",
    "clean_text = demoji.replace(all_text,\"\")\n",
    "#remove smart quotes\n",
    "clean_text = clean_text.replace(\"“\", \"\\\"\").replace(\"”\",\"\\\"\")\n",
    "# convert text to lower-case\n",
    "clean_text = clean_text.lower()\n",
    "\n",
    "spell = SpellChecker()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Find and correct spelling errors\n",
    "corrected_text = []\n",
    "clean_text=clean_text.split()\n",
    "for word in clean_text:\n",
    "    # Check if the word is misspelled\n",
    "    if spell.unknown([word]):\n",
    "        # Get the corrected version of the word\n",
    "        corrected_word = spell.correction(word)\n",
    "        # Check if the corrected word is not None\n",
    "        if corrected_word is not None:\n",
    "            corrected_text.append(corrected_word)\n",
    "        else:\n",
    "            # If the correction is None, keep the original word\n",
    "            corrected_text.append(word)\n",
    "    else:\n",
    "        corrected_text.append(word)\n",
    "# Join the corrected words back into a string\n",
    "corrected_text = \" \".join(corrected_text)   \n",
    "\n",
    "#Tokenzing using Spacy with removing white spaces, stop words, and punctuations\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(corrected_text)\n",
    "\n",
    "# Lemmatize and stem the words\n",
    "lemmatized_and_stemmed_words = []\n",
    "for token in doc:\n",
    "    lemma = token.lemma_\n",
    "    stem = stemmer.stem(token.text)  # Use Porter Stemmer\n",
    "    lemmatized_and_stemmed_words.append((token.text, lemma, stem))\n",
    "\n",
    "clean_words = [token.text for token in doc if not (token.is_space or token.is_stop or token.is_punct)]\n",
    "posArray = [(token.text, token.pos_) for token in doc if not (token.is_space or token.is_stop or token.is_punct)]\n",
    "\n",
    "# print(clean_words)\n",
    "with open(\"cleaned_data.txt\", \"w\", encoding='utf-8') as txt_file:\n",
    "     txt_file.write(str(clean_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aac27d-9c3a-4a5c-9823-b3c46c114253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Performing a binary classification related to Stockton housing or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "515f8688-635b-4178-828b-7759a90f6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your JSON data\n",
    "path= \"faqsFromPdfCh9.json\"\n",
    "with open(path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract questions and labels\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "for category, faqs in data.items():\n",
    "    for faq in faqs:\n",
    "        questions.append(faq['question'])\n",
    "        # Set the label to 1 if it's related to Stockton housing, else set it to 0\n",
    "        labels.append(1 if category == 'Stockton-Housing' else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(questions, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6d494-c830-4b0b-ab71-5000cbb0e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Computing TF-IDF vectors on the text data\n",
    "# Creating the vectorizer and then using the vectorizer to Convert the text to TF-IDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e524338-50d2-46c5-8910-5dbdfade175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer to convert text into a numerical format\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5cb71-acf7-4dd6-ba0d-3d189a819901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fc1fe-9d5f-4a95-bd04-c1ed71eaa1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Binary classification problem with the Naïve Bayes classifier.\n",
    "# Classifying the documents into stockton-housing and non-stockton-housing with multinomial Naïve Bayes from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fdda4a9-1de2-4dfd-8ae1-fc69641b7cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the naive bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Initialize the classifier and train it\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ce70d-0e72-40b3-bf48-ed454a1f577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding accuracy based on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49c11735-069e-446a-a5e1-76e625be6a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01e517f5-d833-4ad3-b181-b6b3e2bce759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  0]\n",
      " [ 1  2]]\n"
     ]
    }
   ],
   "source": [
    "# View the results as a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred,normalize=None)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b0b84-9947-4b18-8c0b-5d8cb6ed3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  5.Binary classification problem with the SVC classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "416c0b3a-4e29-476a-8d28-d05aea8020d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Train an SVC classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# classifier = SVC()\n",
    "# Create a pipeline with a TF-IDF vectorizer and a linear SVC classifier\n",
    "svc_tfidf = Pipeline([\n",
    "    (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\", max_features=100)),\n",
    "    (\"linear_svc\", SVC(kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "svc_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels on the test set\n",
    "y_pred = svc_tfidf.predict(X_test)\n",
    "\n",
    "# classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict labels on the test set\n",
    "# y_pred = classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd0202db-23c1-42fc-a8e4-617a4967b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  0]\n",
      " [ 2  1]]\n"
     ]
    }
   ],
   "source": [
    "# View the results as a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred,normalize=None)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82838410-c1e0-4423-8d49-cfbce494fff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
