{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5197f-14ca-4796-81e7-93a5c52e4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-1: Cleaning the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e54409b2-30c2-4350-9375-a50fdf9ab10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data file by removing non-text (e.g. emojis, smart quotes) and regularizing text \n",
    "#(e.g. tokenization, lower casing, stemming, lemmatizing, POS tagging, stop word removal, removing punctuation, spelling correction)\n",
    "import demoji\n",
    "import json\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "path= \"faqsFromPdf.json\"\n",
    "with open(path, 'r') as json_file:\n",
    "    text = json.load(json_file)\n",
    "\n",
    "categories = []\n",
    "all_text = \"\"\n",
    "for key, faq_list in text.items():\n",
    "    group_text = \"\"\n",
    "\n",
    "    for faq_item in faq_list:\n",
    "        all_text += faq_item[\"question\"] + \" \" + faq_item[\"answer\"] + \" \"\n",
    "        group_text += faq_item[\"question\"] + faq_item[\"answer\"] \n",
    "    categories.append(group_text)\n",
    "\n",
    "# Removing emojis\n",
    "clean_text = demoji.replace(all_text,\"\")\n",
    "#remove smart quotes\n",
    "clean_text = clean_text.replace(\"“\", \"\\\"\").replace(\"”\",\"\\\"\")\n",
    "# convert text to lower-case\n",
    "clean_text = clean_text.lower()\n",
    "\n",
    "spell = SpellChecker()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Find and correct spelling errors\n",
    "corrected_text = []\n",
    "clean_text=clean_text.split()\n",
    "for word in clean_text:\n",
    "    # Check if the word is misspelled\n",
    "    if spell.unknown([word]):\n",
    "        # Get the corrected version of the word\n",
    "        corrected_word = spell.correction(word)\n",
    "        # Check if the corrected word is not None\n",
    "        if corrected_word is not None:\n",
    "            corrected_text.append(corrected_word)\n",
    "        else:\n",
    "            # If the correction is None, keep the original word\n",
    "            corrected_text.append(word)\n",
    "    else:\n",
    "        corrected_text.append(word)\n",
    "# Join the corrected words back into a string\n",
    "corrected_text = \" \".join(corrected_text)   \n",
    "\n",
    "#Tokenzing using Spacy with removing white spaces, stop words, and punctuations\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(corrected_text)\n",
    "\n",
    "# Lemmatize and stem the words\n",
    "lemmatized_and_stemmed_words = []\n",
    "for token in doc:\n",
    "    lemma = token.lemma_\n",
    "    stem = stemmer.stem(token.text)  # Use Porter Stemmer\n",
    "    lemmatized_and_stemmed_words.append((token.text, lemma, stem))\n",
    "\n",
    "clean_words = [token.text for token in doc if not (token.is_space or token.is_stop or token.is_punct)]\n",
    "posArray = [(token.text, token.pos_) for token in doc if not (token.is_space or token.is_stop or token.is_punct)]\n",
    "\n",
    "# print(clean_words)\n",
    "with open(\"cleaned_data.txt\", \"w\", encoding='utf-8') as txt_file:\n",
    "     txt_file.write(str(clean_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea834dc7-2a7f-462e-8e0e-7f2fdf942b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-2: Counting BoW on pre-processed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "46670775-2b67-43c4-b716-3257c71cd9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   program  students  housing  campus  pacific  yellow  ribbon  units  \\\n",
      "0       63        18        0       2        4       0       0      4   \n",
      "1        0         0        0       0        0       0       0      0   \n",
      "2        4        20       23       9        2       0       0     10   \n",
      "3        1         1        0       0        0       0       0      2   \n",
      "4        0        18       33      59        4       0       0     11   \n",
      "5        8         4        0       0        0       0       0      2   \n",
      "6        1         1        0       0        0       0       0     13   \n",
      "\n",
      "   available  student  ...  law  behalf  certification  following  trademark  \\\n",
      "0         18       24  ...    0       0              0          0          0   \n",
      "1          0        0  ...    0       0              0          0          0   \n",
      "2         14       34  ...    0       0              0          0          0   \n",
      "3          0        3  ...    0       0              0          0          0   \n",
      "4          7       20  ...    0       0              0          0          0   \n",
      "5          2        7  ...    2       2              2          2          2   \n",
      "6          0        1  ...    0       0              0          0          0   \n",
      "\n",
      "   floor  academics  22  33                                           category  \n",
      "0      0          0   1   1  How long are the programs?How long are the pro...  \n",
      "1      0          0   0   0  What kind of clothes can I bring to swap? You ...  \n",
      "2      0          0   0   0  What is my campus address, and how do I receiv...  \n",
      "3      0          0   2   0  How do I get certi�ed i e have my enrollment r...  \n",
      "4      1          0   0   0  How do I request/apply for housing? In order t...  \n",
      "5      0          0   2   0  What is the Yellow Ribbon Program? The Yellow ...  \n",
      "6      0          0   2   0  What is a Transfer Admission Agreement? A Tran...  \n",
      "\n",
      "[7 rows x 1001 columns]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Extract the most common words\n",
    "all_words = nltk.FreqDist(clean_words)\n",
    "max_words = 1000\n",
    "word_features = [word for word, _ in all_words.most_common(max_words)]\n",
    "\n",
    "def document_features(document):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        # print(document)\n",
    "        features[word] = document.count(word)\n",
    "    return features\n",
    "\n",
    "\n",
    "# Create a list of documents as pairs of (text, category)\n",
    "documents = [(text, category) for text, category in zip(clean_words, categories)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Collect features for each document\n",
    "featuresets = [(document_features(category), category) for word, category in documents]\n",
    "\n",
    "# Convert featuresets to a DataFrame and save to a CSV file for Count BoW\n",
    "df_featuresets = pd.DataFrame([features for features, _ in featuresets], columns=word_features)\n",
    "df_featuresets['category'] = [category for _, category in featuresets]\n",
    "df_featuresets.to_csv('count_bow.csv', index=False)\n",
    "\n",
    "# Display the Count BoW representation\n",
    "print(df_featuresets.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4d3ab-c1f0-490d-a80c-30169215bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-3: Computing TF-IDF vectors on pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2baab95b-9b11-48db-9c74-4d121bb4fe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         11      able  academic  admission  admissions  agreement       aid  \\\n",
      "0  0.000000  0.025422  0.000000   0.000000    0.000000   0.000000  0.000000   \n",
      "1  0.007967  0.013834  0.055337   0.214378    0.027668   0.000000  0.095605   \n",
      "2  0.019874  0.069018  0.051763   0.000000    0.000000   0.129179  0.000000   \n",
      "3  0.000000  0.166770  0.000000   0.000000    0.000000   0.000000  0.000000   \n",
      "4  0.000000  0.000000  0.040029   0.215755    0.120086   0.138315  0.000000   \n",
      "5  0.000000  0.000000  0.000000   0.000000    0.338568   0.000000  0.129987   \n",
      "6  0.096232  0.000000  0.033420   0.000000    0.033420   0.019246  0.019246   \n",
      "\n",
      "   apartments  applicants  application  ...  transferring   tuition     units  \\\n",
      "0    0.051384    0.000000     0.000000  ...      0.000000  0.000000  0.107816   \n",
      "1    0.000000    0.157202     0.127474  ...      0.000000  0.031868  0.021335   \n",
      "2    0.151128    0.000000     0.019874  ...      0.000000  0.000000  0.079830   \n",
      "3    0.000000    0.000000     0.000000  ...      0.000000  0.000000  0.000000   \n",
      "4    0.000000    0.000000     0.000000  ...      0.422368  0.046105  0.200632   \n",
      "5    0.000000    0.000000     0.000000  ...      0.000000  0.000000  0.087024   \n",
      "6    0.000000    0.000000     0.096232  ...      0.000000  0.057739  0.012885   \n",
      "\n",
      "   university        va  veterans      work      year    yellow       yes  \n",
      "0    0.019603  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1    0.064006  0.000000  0.000000  0.242341  0.036355  0.000000  0.015934  \n",
      "2    0.079830  0.000000  0.000000  0.011625  0.030229  0.000000  0.079495  \n",
      "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4    0.061733  0.000000  0.000000  0.000000  0.035064  0.000000  0.230525  \n",
      "5    0.043512  0.456220  0.076037  0.000000  0.049429  0.228110  0.000000  \n",
      "6    0.012885  0.292716  0.180133  0.000000  0.029275  0.517882  0.000000  \n",
      "\n",
      "[7 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute TF-IDF vectors on pre-processed data.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Initialize the TF-IDF vectorizer with parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit and transform the corpus to obtain TF-IDF features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(categories)\n",
    "\n",
    "# Get the TF-IDF feature names (words)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array for easier manipulation\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# Create a DataFrame to display the TF-IDF values\n",
    "import pandas as pd\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_dense, columns=tfidf_feature_names)\n",
    "\n",
    "print(tfidf_df)\n",
    "\n",
    "# Save the TF-IDF data to a CSV file\n",
    "tfidf_df.to_csv('tfidf_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c8fd3-b804-4818-a42d-1e555e47ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-4: Perform integer encoding and one-hot encoding on one of the pre-processed data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d450a8c4-6265-4c89-b6de-56b8c99afac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 20  9 27 23 20  9 27 22  2  4  5  1  0 21 17 15 25 28 30  7 26 10  6\n",
      "  2  4  5  1  0 21 17 15 25 28 30  7 26 10  6 12 18 11  8 16 29 24 13 14\n",
      " 19  8]\n"
     ]
    }
   ],
   "source": [
    "# Perform integer encoding\n",
    "import numpy \n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# for this example, we'll just look at the first document, and \n",
    "# the first 50 words\n",
    "data = documents[0][1]\n",
    "# print(data)\n",
    "# Split the text into words\n",
    "values = data.split()\n",
    "# print(values)\n",
    "short_values = (values[:50])\n",
    "\n",
    "# first encode words as integers\n",
    "# every word in the vocabulary gets a unique number\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(short_values)\n",
    "\n",
    "# look at the first 50 encodings\n",
    "print(integer_encoded)\n",
    "\n",
    "with open(\"Integer_Encoding.txt\", 'w') as file:\n",
    "    for integer in integer_encoded:\n",
    "        file.write(str(integer) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "587061f5-4555-4d81-9a5d-f68c5d8f4840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "['How']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# convert the integer encoding to onehot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "print(onehot_encoded)\n",
    "\n",
    "# invert the first vector so that we can see the original word it encodes\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "\n",
    "print(inverted)\n",
    "\n",
    "with open(\"One-hot_Encoding.txt\", 'w') as file:\n",
    "    for encoding in onehot_encoded:\n",
    "        line = \" \".join(map(str, encoding))\n",
    "        file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cca12-7b95-480e-a22e-8edee2a792bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-5: find the words that are the most similar to the word 'campus' it in one of the pre-processed data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "25fa8c94-6057-4d0f-a039-f8d7ca5f01f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow: 0.7226392030715942\n",
      "students: 0.7035462260246277\n",
      "ribbon: 0.6817548871040344\n",
      "station: 0.6632461547851562\n",
      "university: 0.6621088981628418\n",
      "program: 0.6616090536117554\n",
      "housing: 0.6605948209762573\n",
      "available: 0.6577374935150146\n",
      "units: 0.6506310105323792\n",
      "service: 0.6304279565811157\n",
      "pacific: 0.6235244274139404\n",
      "office: 0.62343829870224\n",
      "residential: 0.6178445219993591\n",
      "space: 0.6149711012840271\n",
      "apartments: 0.6142014861106873\n",
      "admissions: 0.6053345799446106\n",
      "sacramento: 0.5987273454666138\n",
      "award: 0.5973877310752869\n",
      "include: 0.5966724157333374\n",
      "field: 0.5951021909713745\n",
      "residence: 0.5939453840255737\n",
      "va: 0.5935418605804443\n",
      "roommate: 0.5931534767150879\n",
      "trimester: 0.5897179245948792\n",
      "number: 0.5890599489212036\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Extract the tokens from the doc object and store them in a list\n",
    "tokens = clean_words\n",
    "\n",
    "# Create and train the Word2Vec model\n",
    "model = gensim.models.Word2Vec(sentences=[tokens], min_count=1, vector_size=100, sg=0)\n",
    "\n",
    "# Find similar words to 'graduate'\n",
    "similar_words = model.wv.most_similar(positive=['campus'], topn=25)\n",
    "\n",
    "# Display the similar words\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c6577-10ad-436f-bed4-0097d4baf87c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
